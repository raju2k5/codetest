import software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider;
import software.amazon.awssdk.regions.Region;
import software.amazon.awssdk.services.s3.S3Client;
import software.amazon.awssdk.services.s3.model.GetObjectRequest;
import software.amazon.awssdk.services.s3.model.PutObjectRequest;
import org.apache.commons.csv.CSVFormat;
import org.apache.commons.csv.CSVParser;
import org.apache.commons.csv.CSVRecord;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.parquet.example.data.Group;
import org.apache.parquet.example.data.simple.SimpleGroupFactory;
import org.apache.parquet.hadoop.ParquetWriter;
import org.apache.parquet.hadoop.example.GroupWriteSupport;
import org.apache.parquet.schema.MessageType;
import org.apache.parquet.schema.MessageTypeParser;

import java.io.*;
import java.util.ArrayList;
import java.util.List;

public class S3CsvToParquetConverter {

    public static void main(String[] args) throws IOException {
        String bucketName = "your-bucket-name";
        String csvKey = "path/to/your/file.csv";
        String parquetKey = "path/to/output/file.parquet";

        // Initialize S3 client
        S3Client s3Client = S3Client.builder()
                .region(Region.US_EAST_1) // Replace with your desired AWS region
                .credentialsProvider(DefaultCredentialsProvider.create())
                .build();

        // Download CSV file from S3 to local temp file
        File tempCsvFile = downloadCsvFromS3(s3Client, bucketName, csvKey);

        // Convert CSV to Parquet
        List<String[]> records = readCsv(tempCsvFile);
        convertCsvToParquet(records, tempCsvFile.getAbsolutePath() + ".parquet");

        // Upload Parquet file to S3
        uploadParquetToS3(s3Client, bucketName, parquetKey, new File(tempCsvFile.getAbsolutePath() + ".parquet"));

        // Delete local temp files if needed
        tempCsvFile.delete();
        new File(tempCsvFile.getAbsolutePath() + ".parquet").delete();

        System.out.println("CSV from S3 converted to Parquet and uploaded back to S3 successfully!");
    }

    public static File downloadCsvFromS3(S3Client s3Client, String bucketName, String csvKey) throws IOException {
        // Prepare a temporary file to store the downloaded CSV
        File tempFile = File.createTempFile("temp", ".csv");

        // Download CSV file from S3 to local temp file
        s3Client.getObject(GetObjectRequest.builder().bucket(bucketName).key(csvKey).build(),
                ResponseTransformer.toFile(tempFile));

        return tempFile;
    }

    public static List<String[]> readCsv(File csvFile) throws IOException {
        List<String[]> records = new ArrayList<>();
        Reader reader = new FileReader(csvFile);
        CSVParser csvParser = new CSVParser(reader, CSVFormat.DEFAULT);

        for (CSVRecord csvRecord : csvParser) {
            records.add(new String[] { csvRecord.get(0), csvRecord.get(1), csvRecord.get(2) });
        }

        csvParser.close();
        reader.close();

        return records;
    }

    public static void convertCsvToParquet(List<String[]> records, String parquetFilePath) throws IOException {
        // Define the Parquet schema
        MessageType schema = MessageTypeParser.parseMessageType(
                "message CsvSchema {" + "  required binary col1;" + "  required int32 col2;" + "  required double col3;"
                        + "}");

        // Configure Parquet writing
        Configuration conf = new Configuration();
        Path path = new Path(parquetFilePath);
        GroupWriteSupport writeSupport = new GroupWriteSupport();
        writeSupport.setSchema(schema, conf);

        ParquetWriter<Group> writer = new ParquetWriter<>(path, (WriteSupport<Group>) writeSupport,
                ParquetWriter.DEFAULT_COMPRESSION_CODEC_NAME, ParquetWriter.DEFAULT_BLOCK_SIZE,
                ParquetWriter.DEFAULT_PAGE_SIZE, ParquetWriter.DEFAULT_PAGE_SIZE,
                ParquetWriter.DEFAULT_IS_DICTIONARY_ENABLED, ParquetWriter.DEFAULT_IS_VALIDATING_ENABLED,
                ParquetWriter.DEFAULT_WRITER_VERSION, conf);

        // Convert CSV records to Parquet Groups and write them
        SimpleGroupFactory groupFactory = new SimpleGroupFactory(schema);
        for (String[] record : records) {
            Group group = groupFactory.newGroup().append("col1", record[0]).append("col2", Integer.parseInt(record[1]))
                    .append("col3", Double.parseDouble(record[2]));

            writer.write(group);
        }

        writer.close();
    }

    public static void uploadParquetToS3(S3Client s3Client, String bucketName, String parquetKey, File parquetFile) {
        // Upload Parquet file to S3
        s3Client.putObject(PutObjectRequest.builder().bucket(bucketName).key(parquetKey).build(),
                Path.fromFile(parquetFile));
    }
}
