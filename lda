
import org.apache.avro.Schema;
import org.apache.avro.generic.GenericData;
import org.apache.avro.generic.GenericRecord;
import org.apache.hadoop.conf.Configuration;
import org.apache.parquet.avro.AvroParquetWriter;
import org.apache.parquet.hadoop.ParquetWriter;
import org.apache.parquet.hadoop.metadata.CompressionCodecName;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.File;
import java.io.IOException;
import java.time.LocalDate;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.Arrays;
import java.util.List;

public File convertCsvToParquet(List<String[]> csvData, Schema avroSchema, String fileName) throws IOException {
    File parquetFile = new File(fileName);

    Configuration hadoopConfig = new Configuration();
    hadoopConfig.set("parquet.native.enabled", "false");

    Logger log = LoggerFactory.getLogger(SnapshotServiceImpl.class);
    log.debug("Starting Parquet file writing. Output file: {}", fileName);

    int totalParquetRecords = 0;

    // Set up the Parquet writer
    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(new org.apache.hadoop.fs.Path(parquetFile.getAbsolutePath()))
            .withSchema(avroSchema)
            .withConf(hadoopConfig)
            .withValidation(false)
            .withCompressionCodec(CompressionCodecName.SNAPPY)
            .build()) {

        // Get the current date and timestamp to insert for missing fields
        String currentDate = LocalDate.now().format(DateTimeFormatter.ofPattern("yyyy-MM-dd"));
        String currentTimestamp = LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyy-MM-dd'T'HH:mm:ss"));

        // Extract headers from the first row of the CSV data
        boolean firstRow = true;
        String[] headers = csvData.get(0);

        for (String[] record : csvData) {
            if (firstRow) {
                firstRow = false;
                continue; // Skip the header row
            }

            // Create a new Avro record based on the Avro schema
            GenericRecord avroRecord = new GenericData.Record(avroSchema);

            // Iterate over the Avro schema fields and populate values from the CSV
            for (Schema.Field field : avroSchema.getFields()) {
                String fieldName = field.name();

                // Find the index of the field in the CSV data using headers
                int columnIndex = Arrays.asList(headers).indexOf(fieldName);

                // If the field is not found in the CSV, handle it accordingly
                if (columnIndex == -1 || columnIndex >= record.length) {
                    // Handle EFF_DT and ETL_TS fields explicitly when they are missing in CSV
                    if ("EFF_DT".equals(fieldName)) {
                        avroRecord.put(fieldName, currentDate);  // Insert current date
                        log.info("Field '{}' not found in CSV. Using current date: {}", fieldName, currentDate);
                    } else if ("ETL_TS".equals(fieldName)) {
                        avroRecord.put(fieldName, currentTimestamp);  // Insert current timestamp
                        log.info("Field '{}' not found in CSV. Using current timestamp: {}", fieldName, currentTimestamp);
                    } else {
                        // For any other fields missing in the CSV, leave them empty or handle null as needed
                        avroRecord.put(fieldName, null);
                    }
                } else {
                    // If the field is present in the CSV, populate its value
                    String value = record[columnIndex];
                    avroRecord.put(fieldName, value);
                }
            }

            // Write the Avro record to the Parquet file
            writer.write(avroRecord);
            totalParquetRecords++;
        }

        log.info("Parquet file written successfully. File: {}", fileName);
        log.info("Total records written to Parquet file: {}", totalParquetRecords);

        // Compare the total records in CSV and Parquet data
        if (totalParquetRecords == (csvData.size() - 1)) {
            log.info("Record count matches between CSV and Parquet files.");
        } else {
            log.warn("Record count mismatch: CSV records = {}, Parquet records = {}", csvData.size() - 1, totalParquetRecords);
        }

    } catch (IOException e) {
        log.error("Error writing Parquet file: {}", e.getMessage(), e);
        throw new IOException("Error converting CSV data to Parquet", e);
    }

    return parquetFile;
}
