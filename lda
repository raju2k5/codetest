<dependencies>
    <dependency>
        <groupId>org.apache.parquet</groupId>
        <artifactId>parquet-avro</artifactId>
        <version>1.12.0</version> <!-- Replace with the latest version -->
    </dependency>
    <dependency>
        <groupId>com.opencsv</groupId>
        <artifactId>opencsv</artifactId>
        <version>5.6</version> <!-- Replace with the latest version -->
    </dependency>
</dependencies>


import com.opencsv.CSVReader;
import com.opencsv.exceptions.CsvException;
import org.apache.avro.Schema;
import org.apache.avro.SchemaBuilder;
import org.apache.avro.generic.GenericData;
import org.apache.avro.generic.GenericRecord;
import org.apache.avro.generic.GenericRecordBuilder;
import org.apache.parquet.avro.AvroParquetWriter;
import org.apache.parquet.hadoop.ParquetWriter;
import org.apache.parquet.hadoop.metadata.CompressionCodecName;

import java.io.FileReader;
import java.io.IOException;
import java.util.List;

public class CsvToParquetConverter {

    public static void main(String[] args) {
        String csvFilePath = "input.csv"; // Path to your CSV file
        String parquetFilePath = "output.parquet"; // Path to output Parquet file

        CsvToParquetConverter converter = new CsvToParquetConverter();
        try {
            converter.convertCsvToParquet(csvFilePath, parquetFilePath);
            System.out.println("Conversion completed successfully.");
        } catch (IOException | CsvException e) {
            e.printStackTrace();
        }
    }

    public void convertCsvToParquet(String csvFilePath, String parquetFilePath) throws IOException, CsvException {
        // Read CSV data
        List<String[]> csvData = readCsv(csvFilePath);

        // Define Avro schema
        Schema avroSchema = buildAvroSchema();

        // Define ParquetWriter
        try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(new org.apache.hadoop.fs.Path(parquetFilePath))
                .withSchema(avroSchema)
                .withConf(new org.apache.hadoop.conf.Configuration())
                .withCompressionCodec(CompressionCodecName.SNAPPY)
                .build()) {

            // Convert CSV data to Avro GenericRecord and write to Parquet
            for (String[] record : csvData) {
                GenericRecord avroRecord = new GenericData.Record(avroSchema);
                // Example: Map CSV columns to Avro fields
                avroRecord.put("col1", record[0]);
                avroRecord.put("col2", Integer.parseInt(record[1]));
                avroRecord.put("col3", Double.parseDouble(record[2]));
                // Add more fields as needed

                writer.write(avroRecord);
            }
        }
    }

    private List<String[]> readCsv(String csvFilePath) throws IOException, CsvException {
        CSVReader csvReader = new CSVReader(new FileReader(csvFilePath));
        List<String[]> csvData = csvReader.readAll();
        csvReader.close();
        return csvData;
    }

    private Schema buildAvroSchema() {
        // Define Avro schema
        Schema schema = SchemaBuilder.record("CsvRecord").namespace("com.example")
                .fields()
                .name("col1").type().stringType().noDefault()
                .name("col2").type().intType().noDefault()
                .name("col3").type().doubleType().noDefault()
                // Add more fields as needed
                .endRecord();
        return schema;
    }
}
