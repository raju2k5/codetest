import com.opencsv.CSVReader;
import com.opencsv.exceptions.CsvException;
import org.apache.avro.Schema;
import org.apache.avro.SchemaBuilder;
import org.apache.avro.generic.GenericData;
import org.apache.avro.generic.GenericRecord;
import org.apache.avro.generic.GenericRecordBuilder;
import org.apache.parquet.avro.AvroParquetWriter;
import org.apache.parquet.hadoop.ParquetWriter;
import org.apache.parquet.hadoop.metadata.CompressionCodecName;

import java.io.*;
import java.util.List;

public class CsvToParquetConverter {

    public static void main(String[] args) {
        String csvFilePath = "path/to/your/csv/file.csv";
        String parquetFilePath = "path/to/output/converted.parquet";

        try {
            // Read CSV file
            List<String[]> csvData = readCsv(csvFilePath);

            // Infer schema and convert CSV to Parquet
            convertCsvToParquet(csvData, parquetFilePath);

            System.out.println("Conversion completed successfully.");

        } catch (IOException | CsvException e) {
            e.printStackTrace();
        }
    }

    private static List<String[]> readCsv(String csvFilePath) throws IOException, CsvException {
        CSVReader csvReader = new CSVReader(new FileReader(csvFilePath));
        List<String[]> csvData = csvReader.readAll();
        csvReader.close();
        return csvData;
    }

    private static void convertCsvToParquet(List<String[]> csvData, String parquetFilePath) throws IOException {
        // Infer Avro schema dynamically from CSV data
        Schema avroSchema = buildAvroSchema(csvData.get(0)); // Assuming the first row contains headers

        // Write Avro records to Parquet
        File parquetFile = new File(parquetFilePath);
        try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(new org.apache.hadoop.fs.Path(parquetFile.getAbsolutePath()))
                .withSchema(avroSchema)
                .withConf(new org.apache.hadoop.conf.Configuration())
                .withCompressionCodec(CompressionCodecName.SNAPPY)
                .build()) {

            // Convert CSV data to Avro GenericRecord and write to Parquet
            for (int i = 1; i < csvData.size(); i++) { // Start from index 1 to skip header row
                String[] record = csvData.get(i);
                GenericRecord avroRecord = convertCsvRecordToAvro(record, avroSchema);
                writer.write(avroRecord);
            }
        }
    }

    private static Schema buildAvroSchema(String[] headerRow) {
        SchemaBuilder.FieldAssembler<Schema> schemaBuilder = SchemaBuilder.record("CsvRecord").namespace("com.example").fields();
        for (String fieldName : headerRow) {
            schemaBuilder = schemaBuilder.name(fieldName.trim()).type().stringType().noDefault();
            // You can infer types more precisely based on the actual data if needed
        }
        return schemaBuilder.endRecord();
    }

    private static GenericRecord convertCsvRecordToAvro(String[] csvRecord, Schema avroSchema) {
        GenericRecordBuilder recordBuilder = new GenericRecordBuilder(avroSchema);
        for (int i = 0; i < csvRecord.length; i++) {
            recordBuilder.set(i, csvRecord[i]);
        }
        return recordBuilder.build();
    }
}
