public File convertCsvToParquet(List<String[]> csvData, Schema avroSchema, String fileName) throws IOException {
    File parquetFile = new File(fileName);

    Configuration hadoopConfig = new Configuration();
    hadoopConfig.set("parquet.native.enabled", "false");

    log.debug("Starting Parquet file writing. Output file: {}", fileName);

    int totalParquetRecords = 0;

    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(new org.apache.hadoop.fs.Path(parquetFile.getAbsolutePath()))
            .withSchema(avroSchema)
            .withConf(hadoopConfig)
            .withValidation(false)
            .withCompressionCodec(CompressionCodecName.SNAPPY)
            .build()) {

        String currentDate = LocalDate.now().format(DateTimeFormatter.ofPattern("yyyy-MM-dd"));
        String currentTimestamp = LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyy-MM-dd'T'HH:mm:ss"));

        boolean firstRow = true;
        String[] headers = csvData.get(0);
        log.debug("CSV Headers: {}", Arrays.toString(headers));

        for (String[] record : csvData) {
            if (firstRow) {
                firstRow = false;
                continue; // Skip the header row
            }

            GenericRecord avroRecord = new GenericData.Record(avroSchema);

            // Always insert the date and timestamp for EFF_DT and ETL_TS manually
            avroRecord.put("EFF_DT", currentDate);
            log.info("Inserted current date '{}' for 'EFF_DT'", currentDate);
            avroRecord.put("ETL_TS", currentTimestamp);
            log.info("Inserted current timestamp '{}' for 'ETL_TS'", currentTimestamp);

            for (Schema.Field field : avroSchema.getFields()) {
                String fieldName = field.name();
                if (!"EFF_DT".equals(fieldName) && !"ETL_TS".equals(fieldName)) {
                    int columnIndex = Arrays.asList(headers).indexOf(fieldName);
                    log.debug("Processing field: {}", fieldName);
                    log.debug("Column index for '{}': {}", fieldName, columnIndex);

                    if (columnIndex == -1 || columnIndex >= record.length) {
                        avroRecord.put(fieldName, null); // Insert null if the field is missing
                        log.info("Field '{}' not found in CSV. Inserting null.", fieldName);
                    } else {
                        String value = record[columnIndex];
                        avroRecord.put(fieldName, value);
                        log.debug("Field '{}' found in CSV with value: {}", fieldName, value);
                    }
                }
            }

            writer.write(avroRecord);
            totalParquetRecords++;
        }

        log.info("Parquet file written successfully. File: {}", fileName);
        log.info("Total records written to Parquet file: {}", totalParquetRecords);

        if (totalParquetRecords == (csvData.size() - 1)) {
            log.info("Record count matches between CSV and Parquet files.");
        } else {
            log.warn("Record count mismatch: CSV records = {}, Parquet records = {}", csvData.size() - 1, totalParquetRecords);
        }

    } catch (IOException e) {
        log.error("Error writing Parquet file: {}", e.getMessage());
        throw new IOException("Error converting CSV data to Parquet", e);
    }

    return parquetFile;
}
