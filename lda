private File convertCsvToParquet(List<String[]> csvData, Schema avroSchema, String fileName) throws IOException {
    File parquetFile = new File(fileName);

    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(new org.apache.hadoop.fs.Path(parquetFile.getAbsolutePath()))
            .withSchema(avroSchema)
            .withConf(new org.apache.hadoop.conf.Configuration())
            .withValidation(false)
            .withCompressionCodec(CompressionCodecName.SNAPPY)
            .build()) {

        // Get the CSV header (first row)
        String[] headers = csvData.get(0);
        boolean firstRow = true;

        for (String[] record : csvData) {
            if (firstRow) {
                firstRow = false;
                continue; // Skip header row
            }

            GenericRecord avroRecord = new GenericData.Record(avroSchema);

            // Map CSV columns to Avro schema fields by name
            for (Schema.Field field : avroSchema.getFields()) {
                String fieldName = field.name();

                // Check if the CSV has this field
                int columnIndex = Arrays.asList(headers).indexOf(fieldName);
                if (columnIndex >= 0 && columnIndex < record.length) {
                    String value = record[columnIndex];
                    avroRecord.put(fieldName, value);
                } else {
                    // Write null if the column is missing
                    avroRecord.put(fieldName, null);
                }
            }

            writer.write(avroRecord);
        }
    } catch (IOException e) {
        logger.error("Error writing Parquet file: {}", e.getMessage());
        throw new IOException("Error converting CSV data to Parquet", e);
    }

    return parquetFile;
}
