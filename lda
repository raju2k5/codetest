<dependency>
    <groupId>software.amazon.awssdk</groupId>
    <artifactId>s3</artifactId>
    <version>2.17.102</version>
</dependency>
<dependency>
    <groupId>software.amazon.awssdk</groupId>
    <artifactId>s3-paginators</artifactId>
    <version>2.17.102</version>
</dependency>
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-common</artifactId>
    <version>3.3.1</version>
</dependency>
<dependency>
    <groupId>org.apache.parquet</groupId>
    <artifactId>parquet-hadoop</artifactId>
    <version>1.12.0</version>
</dependency>
<dependency>
    <groupId>org.apache.commons</groupId>
    <artifactId>commons-csv</artifactId>
    <version>1.9.0</version>
</dependency>




import software.amazon.awssdk.auth.credentials.*;
import software.amazon.awssdk.regions.*;
import software.amazon.awssdk.services.s3.*;
import software.amazon.awssdk.services.s3.model.*;

import org.apache.commons.csv.*;
import org.apache.hadoop.conf.*;
import org.apache.hadoop.fs.*;
import org.apache.parquet.example.data.*;
import org.apache.parquet.example.data.simple.*;
import org.apache.parquet.hadoop.*;
import org.apache.parquet.hadoop.api.*;
import org.apache.parquet.schema.*;

import java.io.*;
import java.util.*;

public class S3CsvToParquetConverter {

    public static void main(String[] args) throws IOException {
        String bucketName = "your-bucket-name";
        String csvKey = "path/to/your/file.csv";
        String parquetKey = "path/to/output/file.parquet";

        // Initialize S3 client
        S3Client s3Client = S3Client.builder()
                .region(Region.US_EAST_1) // Replace with your desired AWS region
                .credentialsProvider(DefaultCredentialsProvider.create())
                .build();

        // Download CSV file from S3 to local temp file
        File tempCsvFile = downloadCsvFromS3(s3Client, bucketName, csvKey);

        // Convert CSV to Parquet
        List<String[]> records = readCsv(tempCsvFile);
        convertCsvToParquet(records, tempCsvFile.getAbsolutePath() + ".parquet");

        // Upload Parquet file to S3
        uploadParquetToS3(s3Client, bucketName, parquetKey, new File(tempCsvFile.getAbsolutePath() + ".parquet"));

        // Delete local temp files if needed
        tempCsvFile.delete();
        new File(tempCsvFile.getAbsolutePath() + ".parquet").delete();

        System.out.println("CSV from S3 converted to Parquet and uploaded back to S3 successfully!");
    }

    public static File downloadCsvFromS3(S3Client s3Client, String bucketName, String csvKey) throws IOException {
        // Prepare a temporary file to store the downloaded CSV
        File tempFile = File.createTempFile("temp", ".csv");

        // Download CSV file from S3 to local temp file
        GetObjectRequest getObjectRequest = GetObjectRequest.builder()
                .bucket(bucketName)
                .key(csvKey)
                .build();

        s3Client.getObject(getObjectRequest, ResponseTransformer.toFile(tempFile));

        return tempFile;
    }

    public static List<String[]> readCsv(File csvFile) throws IOException {
        List<String[]> records = new ArrayList<>();
        Reader reader = new FileReader(csvFile);
        CSVParser csvParser = new CSVParser(reader, CSVFormat.DEFAULT);

        for (CSVRecord csvRecord : csvParser) {
            records.add(csvRecord.values());
        }

        csvParser.close();
        reader.close();

        return records;
    }

    public static void convertCsvToParquet(List<String[]> records, String parquetFilePath) throws IOException {
        // Define the Parquet schema
        MessageType schema = MessageTypeParser.parseMessageType(
                "message CsvSchema {" +
                        "  required binary col1;" +
                        "  required int32 col2;" +
                        "  required double col3;" +
                        "}"
        );

        // Configure Parquet writing
        Configuration conf = new Configuration();
        Path path = new Path(parquetFilePath);
        GroupWriteSupport writeSupport = new GroupWriteSupport();
        writeSupport.setSchema(schema, conf);

        ParquetWriter<Group> writer = new ParquetWriter<>(
                path,
                (WriteSupport<Group>) writeSupport,
                ParquetWriter.DEFAULT_COMPRESSION_CODEC_NAME,
                ParquetWriter.DEFAULT_BLOCK_SIZE,
                ParquetWriter.DEFAULT_PAGE_SIZE,
                ParquetWriter.DEFAULT_PAGE_SIZE,
                ParquetWriter.DEFAULT_IS_DICTIONARY_ENABLED,
                ParquetWriter.DEFAULT_IS_VALIDATING_ENABLED,
                ParquetWriter.DEFAULT_WRITER_VERSION,
                conf
        );

        // Convert CSV records to Parquet Groups and write them
        SimpleGroupFactory groupFactory = new SimpleGroupFactory(schema);
        for (String[] record : records) {
            Group group = groupFactory.newGroup()
                    .append("col1", record[0])
                    .append("col2", Integer.parseInt(record[1]))
                    .append("col3", Double.parseDouble(record[2]));

            writer.write(group);
        }

        writer.close();
    }

    public static void uploadParquetToS3(S3Client s3Client, String bucketName, String parquetKey, File parquetFile) {
        // Upload Parquet file to S3
        PutObjectRequest putObjectRequest = PutObjectRequest.builder()
                .bucket(bucketName)
                .key(parquetKey)
                .build();

        s3Client.putObject(putObjectRequest, Path.fromFile(parquetFile));

        // Alternatively, you can use TransferManager for large files and better performance
//        TransferManager transferManager = TransferManager.builder()
//                .s3Client(s3Client)
//                .build();
//
//        Upload upload = transferManager.upload(bucketName, parquetKey, parquetFile);
//        try {
//            upload.waitForCompletion();
//        } catch (InterruptedException e) {
//            e.printStackTrace();
//        }
//        transferManager.shutdownNow();
    }
}
