import software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider;
import software.amazon.awssdk.regions.Region;
import software.amazon.awssdk.services.s3.S3Client;
import software.amazon.awssdk.services.s3.model.*;

import com.opencsv.CSVReader;
import com.opencsv.exceptions.CsvException;
import org.apache.avro.Schema;
import org.apache.avro.SchemaBuilder;
import org.apache.avro.generic.GenericData;
import org.apache.avro.generic.GenericRecord;
import org.apache.avro.generic.GenericRecordBuilder;
import org.apache.parquet.avro.AvroParquetWriter;
import org.apache.parquet.hadoop.ParquetWriter;
import org.apache.parquet.hadoop.metadata.CompressionCodecName;

import java.io.*;
import java.util.List;
import java.util.stream.Collectors;

public class CsvToParquetS3Converter {

    private final S3Client s3Client;

    public CsvToParquetS3Converter() {
        // Initialize S3 client
        this.s3Client = S3Client.builder()
                .region(Region.US_EAST_1)  // Replace with your desired AWS region
                .credentialsProvider(DefaultCredentialsProvider.create())
                .build();
    }

    public static void main(String[] args) {
        String bucketName = "your-s3-bucket-name";
        String csvKey = "input.csv"; // Path to CSV file in S3
        String parquetKey = "output.parquet"; // Path to output Parquet file in S3

        CsvToParquetS3Converter converter = new CsvToParquetS3Converter();
        try {
            converter.convertCsvToParquetAndUpload(bucketName, csvKey, parquetKey);
            System.out.println("Conversion completed successfully.");
        } catch (IOException | CsvException | SdkClientException e) {
            e.printStackTrace();
        }
    }

    public void convertCsvToParquetAndUpload(String bucketName, String csvKey, String parquetKey)
            throws IOException, CsvException, SdkClientException {
        // Read CSV data from S3
        List<String[]> csvData = readCsvFromS3(bucketName, csvKey);

        // Define Avro schema
        Schema avroSchema = buildAvroSchema();

        // Convert CSV data to Parquet
        File parquetFile = convertCsvToParquet(csvData, avroSchema);

        // Upload Parquet file to S3
        uploadFileToS3(bucketName, parquetKey, parquetFile);

        // Clean up: delete temporary files
        parquetFile.delete();
    }

    private List<String[]> readCsvFromS3(String bucketName, String key) throws IOException, CsvException, SdkClientException {
        // Initialize the S3 getObjectRequest
        GetObjectRequest getObjectRequest = GetObjectRequest.builder()
                .bucket(bucketName)
                .key(key)
                .build();

        // Read CSV data from S3 directly
        ResponseInputStream<GetObjectResponse> responseInputStream = s3Client.getObject(getObjectRequest, ResponseTransformer.toInputStream());

        // Use try-with-resources to ensure stream closure
        try (InputStreamReader inputStreamReader = new InputStreamReader(responseInputStream);
             CSVReader csvReader = new CSVReader(inputStreamReader)) {
            // Read all CSV rows and collect them into a List<String[]>
            List<String[]> csvData = csvReader.readAll();
            return csvData;
        }
    }

    private Schema buildAvroSchema() {
        // Define Avro schema
        Schema schema = SchemaBuilder.record("CsvRecord").namespace("com.example")
                .fields()
                .name("col1").type().stringType().noDefault()
                .name("col2").type().intType().noDefault()
                .name("col3").type().doubleType().noDefault()
                // Add more fields as needed
                .endRecord();
        return schema;
    }

    private File convertCsvToParquet(List<String[]> csvData, Schema avroSchema) throws IOException {
        File parquetFile = File.createTempFile("output", ".parquet");
        try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(new org.apache.hadoop.fs.Path(parquetFile.getAbsolutePath()))
                .withSchema(avroSchema)
                .withConf(new org.apache.hadoop.conf.Configuration())
                .withCompressionCodec(CompressionCodecName.SNAPPY)
                .build()) {

            // Convert CSV data to Avro GenericRecord and write to Parquet
            for (String[] record : csvData) {
                GenericRecord avroRecord = new GenericData.Record(avroSchema);
                avroRecord.put("col1", record[0]);
                avroRecord.put("col2", Integer.parseInt(record[1]));
                avroRecord.put("col3", Double.parseDouble(record[2]));
                // Add more fields if there are more columns in CSV

                writer.write(avroRecord);
            }
        }
        return parquetFile;
    }

    private void uploadFileToS3(String bucketName, String key, File file) throws SdkClientException {
        // Initialize the S3 putObjectRequest
        PutObjectRequest putObjectRequest = PutObjectRequest.builder()
                .bucket(bucketName)
                .key(key)
                .build();

        // Upload file to S3
        s3Client.putObject(putObjectRequest, file.toPath());
    }
}
