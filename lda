import com.opencsv.CSVReader;
import com.opencsv.exceptions.CsvException;
import org.apache.avro.Schema;
import org.apache.avro.generic.GenericData;
import org.apache.avro.generic.GenericRecord;
import org.apache.parquet.avro.AvroParquetWriter;
import org.apache.parquet.hadoop.ParquetWriter;
import org.apache.parquet.hadoop.metadata.CompressionCodecName;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;
import software.amazon.awssdk.core.ResponseInputStream;
import software.amazon.awssdk.core.exception.SdkClientException;
import software.amazon.awssdk.services.s3.S3Client;
import software.amazon.awssdk.services.s3.model.*;

import java.io.*;
import java.util.Arrays;
import java.util.List;

@Service
public class S3Service {
    private static final Logger logger = LoggerFactory.getLogger(S3Service.class);
    private final S3Client s3Client;

    @Autowired
    public S3Service(S3Client s3Client) {
        this.s3Client = s3Client;
    }

    /**
     * Reads CSV data from S3 bucket, converts it to Parquet format,
     * and uploads the Parquet file to S3.
     *
     * @param bucketName S3 bucket name
     * @param fileKey    S3 file key (object key)
     * @param jsonSchema JSON schema in String format
     * @throws IOException  If there's an I/O issue
     * @throws CsvException If there's an issue with CSV parsing
     */
    public void convertCsvToParquetAndUpload(String bucketName, String fileKey, String jsonSchema) throws IOException, CsvException {
        System.setProperty("hadoop.home.dir", "I:\\hadoop");
        String tempFileName = "output_" + System.currentTimeMillis() + ".parquet";

        try {
            // Read CSV data from S3
            List<String[]> csvData = readCsvFromS3(bucketName, fileKey);

            // Build Avro schema from JSON schema
            Schema avroSchema = buildAvroSchema(jsonSchema);

            // Convert CSV data to Parquet
            File parquetFile = convertCsvToParquet(csvData, avroSchema, tempFileName);

            // Upload Parquet file to S3
            String parquetFileKey = fileKey.replaceAll("\\.\\w+", "") + ".parquet";
            uploadParquetToS3(bucketName, parquetFileKey, parquetFile, tempFileName);
        } catch (SdkClientException e) {
            logger.error("AWS SDK Client error: {}", e.getMessage());
            throw e;
        } catch (IOException | CsvException e) {
            logger.error("Error during processing: {}", e.getMessage());
            throw e;
        }
    }

    /**
     * Reads CSV data from S3 into a list of String arrays.
     *
     * @param bucketName S3 bucket name
     * @param key        S3 file key (object key)
     * @return List of String arrays representing CSV rows
     * @throws IOException If there's an I/O issue
     */
    private List<String[]> readCsvFromS3(String bucketName, String key) throws IOException {
        GetObjectRequest getObjectRequest = GetObjectRequest.builder()
                .bucket(bucketName)
                .key(key)
                .build();

        try (ResponseInputStream<GetObjectResponse> objectStream = s3Client.getObject(getObjectRequest);
             Reader reader = new InputStreamReader(objectStream);
             CSVReader csvReader = new CSVReader(reader)) {

            return csvReader.readAll();
        } catch (CsvException e) {
            logger.error("Error reading CSV data: {}", e.getMessage());
            throw new IOException("Error reading CSV data from S3", e);
        } catch (SdkClientException e) {
            logger.error("Error fetching CSV data from S3: {}", e.getMessage());
            throw new IOException("Error fetching CSV data from S3", e);
        }
    }

    /**
     * Builds Avro schema from a provided JSON schema string.
     *
     * @param jsonSchema JSON schema in String format
     * @return Avro Schema object
     */
    private Schema buildAvroSchema(String jsonSchema) {
        Schema.Parser parser = new Schema.Parser();
        return parser.parse(jsonSchema);
    }

    /**
     * Converts CSV data to Parquet format using provided Avro schema and returns as bytes.
     *
     * @param csvData    List of String arrays representing CSV rows
     * @param avroSchema Avro Schema object
     * @return Byte array representing the converted Parquet file
     * @throws IOException If there's an I/O issue
     */
    private File convertCsvToParquet(List<String[]> csvData, Schema avroSchema, String fileName) throws IOException {
        File parquetFile = new File(fileName);

        try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(new org.apache.hadoop.fs.Path(parquetFile.getAbsolutePath()))
                .withSchema(avroSchema)
                .withConf(new org.apache.hadoop.conf.Configuration())
                .withValidation(false)
                .withCompressionCodec(CompressionCodecName.SNAPPY)
                .build()) {

            // Skip the first row (header row)
            boolean firstRow = true;

            for (String[] record : csvData) {
                if (firstRow) {
                    firstRow = false;
                    continue; // Skip header row
                }

                GenericRecord avroRecord = new GenericData.Record(avroSchema);

                // Map CSV columns to Avro schema fields by name
                for (Schema.Field field : avroSchema.getFields()) {
                    String fieldName = field.name();

                    // Assuming CSV headers match Avro field names
                    int columnIndex = Arrays.asList(csvData.get(0)).indexOf(fieldName);
                    String value = record[columnIndex];

                    // Add value to Avro record
                    avroRecord.put(fieldName, value);
                }

                writer.write(avroRecord);
            }
        } catch (IOException e) {
            logger.error("Error writing Parquet file: {}", e.getMessage());
            throw new IOException("Error converting CSV data to Parquet", e);
        }

        return parquetFile;
    }

    /**
     * Uploads a Parquet file to an S3 bucket.
     *
     * @param bucketName S3 bucket name
     * @param key        S3 file key (object key)
     * @param parquetData File representing Parquet data
     * @throws SdkClientException If there's an issue with S3 operations
     */
    private void uploadParquetToS3(String bucketName, String key, File parquetData, String tempFileName) {
        try {
            s3Client.putObject(PutObjectRequest.builder()
                    .bucket(bucketName)
                    .key(key)
                    .build(),
                    RequestBody.fromFile(parquetData));
            deleteSysParquetFile(tempFileName);
        } catch (SdkClientException e) {
            logger.error("Error uploading Parquet file to S3: {}", e.getMessage());
            throw new RuntimeException("Error uploading Parquet file to S3", e);
        }
    }

    private void deleteSysParquetFile(String parquetFileName) {
        File localParquetFile = new File(parquetFileName);
        File localCrcFile = new File("." + parquetFileName + ".crc");
        if (localParquetFile.exists() && localParquetFile.delete()) {
            logger.info("Parquet file deleted successfully");
        } else {
            logger.warn("Failed to delete Parquet file");
        }
        if (localCrcFile.exists() && localCrcFile.delete()) {
            logger.info(".crc file deleted successfully");
        } else {
            logger.warn("Failed to delete .crc file");
        }
    }
}



LAMBDA HANDLER: 
import com.amazonaws.services.lambda.runtime.Context;
import com.amazonaws.services.lambda.runtime.RequestHandler;
import com.amazonaws.services.lambda.runtime.LambdaLogger;
import com.opencsv.exceptions.CsvException;
import software.amazon.awssdk.services.s3.S3Client;

import java.io.IOException;
import java.util.Map;

public class S3ServiceLambdaHandler implements RequestHandler<Map<String, String>, String> {

    @Override
    public String handleRequest(Map<String, String> event, Context context) {
        LambdaLogger logger = context.getLogger();
        String bucketName = event.get("bucketName");
        String fileKey = event.get("fileKey");
        String jsonSchema = event.get("jsonSchema");

        S3Client s3Client = S3Client.create();
        S3Service s3Service = new S3Service(s3Client);

        try {
            s3Service.convertCsvToParquetAndUpload(bucketName, fileKey, jsonSchema);
            return "Conversion and upload successful!";
        } catch (IOException | CsvException e) {
            logger.log("Error during processing: " + e.getMessage());
            return "Error during processing: " + e.getMessage();
        }
    }
}





To add this Lambda function as a module in your existing Terraform setup, you'll follow these steps:

    Create the Lambda Module: Define the new Lambda function in a separate module.
    Update the Main Configuration: Integrate the new module into your main Terraform configuration.

Here's a detailed guide to help you through this process.
Step 1: Create the Lambda Module

Create a new directory for your Lambda module, for example, lambda_csv_to_parquet.

sh

mkdir -p modules/lambda_csv_to_parquet

Inside this directory, create the necessary Terraform configuration files.
File: modules/lambda_csv_to_parquet/main.tf

hcl

resource "aws_iam_role" "lambda_role" {
  name = "lambda-s3-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Action = "sts:AssumeRole",
        Effect = "Allow",
        Principal = {
          Service = "lambda.amazonaws.com",
        },
      },
    ],
  })
}

resource "aws_iam_policy" "lambda_policy" {
  name        = "lambda_s3_policy"
  description = "IAM policy for S3 access from Lambda"
  policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Effect   = "Allow",
        Action   = ["logs:CreateLogGroup", "logs:CreateLogStream", "logs:PutLogEvents"],
        Resource = "arn:aws:logs:*:*:*",
      },
      {
        Effect   = "Allow",
        Action   = ["s3:GetObject", "s3:PutObject", "s3:ListBucket"],
        Resource = [
          "arn:aws:s3:::${var.s3_bucket_name}",
          "arn:aws:s3:::${var.s3_bucket_name}/*",
        ],
      },
    ],
  })
}

resource "aws_iam_role_policy_attachment" "lambda_role_policy_attach" {
  role       = aws_iam_role.lambda_role.name
  policy_arn = aws_iam_policy.lambda_policy.arn
}

resource "aws_lambda_function" "s3_service_lambda" {
  function_name = "CsvToParquetFunction"
  role          = aws_iam_role.lambda_role.arn
  handler       = "S3ServiceLambdaHandler::handleRequest"
  runtime       = "java11"
  timeout       = 300

  s3_bucket = var.lambda_s3_bucket
  s3_key    = var.lambda_s3_key

  environment {
    variables = {
      BUCKET_NAME = var.s3_bucket_name
    }
  }
}

resource "aws_lambda_permission" "allow_s3" {
  statement_id  = "AllowExecutionFromS3"
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.s3_service_lambda.function_name
  principal     = "s3.amazonaws.com"

  source_arn = "arn:aws:s3:::${var.s3_bucket_name}"
}

resource "aws_s3_bucket_notification" "bucket_notification" {
  bucket = var.s3_bucket_name

  lambda_function {
    lambda_function_arn = aws_lambda_function.s3_service_lambda.arn
    events              = ["s3:ObjectCreated:*"]
    filter_prefix       = var.s3_key_prefix
  }
}

File: modules/lambda_csv_to_parquet/variables.tf

hcl

variable "lambda_s3_bucket" {
  description = "S3 bucket where the Lambda JAR is stored"
  type        = string
}

variable "lambda_s3_key" {
  description = "Path to the Lambda JAR file in the S3 bucket"
  type        = string
}

variable "s3_bucket_name" {
  description = "Name of the S3 bucket containing the CSV files"
  type        = string
}

variable "s3_key_prefix" {
  description = "Prefix for CSV files in the S3 bucket"
  type        = string
}

File: modules/lambda_csv_to_parquet/outputs.tf

hcl

output "lambda_function_name" {
  value = aws_lambda_function.s3_service_lambda.function_name
}

output "lambda_function_arn" {
  value = aws_lambda_function.s3_service_lambda.arn
}

Step 2: Update the Main Configuration

Update your main Terraform configuration to include the new Lambda module.
File: main.tf

In your existing main.tf, add the following:

hcl

module "lambda_csv_to_parquet" {
  source = "./modules/lambda_csv_to_parquet"

  lambda_s3_bucket = "your-lambda-bucket"  # S3 bucket where the JAR file is stored
  lambda_s3_key    = "path/to/your/jar-file.jar"  # Path to the JAR file
  s3_bucket_name   = "your-csv-bucket-name"  # S3 bucket containing CSV files
  s3_key_prefix    = "path/to/csv/files/"  # Prefix for CSV files in the S3 bucket
}

Make sure to replace the placeholder values with your actual bucket names and paths.
Step 3: Deploy Using Terraform

    Initialize Terraform:

    sh

terraform init

Apply the Terraform Configuration:

sh

    terraform apply

    Review the changes and type yes to confirm.

Step 4: Verify the Deployment

    Check AWS Lambda Console: Verify that the new Lambda function has been created.
    Upload a CSV File: Upload a CSV file to the specified S3 bucket and prefix to trigger the Lambda function.
    Check CloudWatch Logs: Ensure that the Lambda function executed successfully by checking the CloudWatch logs.

With these steps, you can integrate the new Lambda function into your existing Terraform setup as a module, ensuring it follows your organization's rules and practices.
