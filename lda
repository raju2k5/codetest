  public void convertCsvToParquetAndUpload(String sourceBucketName, String sourceFileKey, String fileTobeProcessed, String destinationBucketName, String destinationFileKy) throws IOException {
        String tempFileName = "/tmp/output_" + System.currentTimeMillis() + ".parquet";

        log.info("Starting the conversion of CSV to Parquet and upload process. Source: {}/{} -> Destination: {}/{}", 
                 sourceBucketName, sourceFileKey, destinationBucketName, destinationFileKy);

        try {
            // Read CSV data from S3
            log.debug("Reading CSV data from S3 bucket: {}, key: {}", sourceBucketName, sourceFileKey);
            List<String[]> csvData = readCsvFromS3(sourceBucketName, sourceFileKey);

            int totalCsvRecords = csvData.size() - 1; // Subtracting 1 for header row
            log.info("Total records in CSV data (excluding header): {}", totalCsvRecords);

            // Load the JSON schema based on fileTobeProcessed
            log.debug("Loading JSON schema for file type: {}", fileTobeProcessed);
            String jsonSchema = loadJsonSchema(fileTobeProcessed);
            Schema avroSchema = new Schema.Parser().parse(jsonSchema);

            // Convert CSV data to Parquet
            log.debug("Converting CSV data to Parquet format.");
            File parquetFile = convertCsvToParquet(csvData, avroSchema, tempFileName);

            // Upload Parquet file to S3
            log.debug("Uploading Parquet file to S3 bucket: {}, key: {}", destinationBucketName, destinationFileKey);
            String destinationFileKey = destinationFileKy.replaceAll("\\.\\w+", "") + ".parquet";
            uploadParquetToS3(destinationBucketName, destinationFileKey, parquetFile, tempFileName);

            log.info("CSV to Parquet conversion and upload completed successfully.");
        } catch (SdkClientException e) {
            log.error("AWS SDK Client error during CSV to Parquet conversion and upload: {}", e.getMessage(), e);
            throw e;
        } catch (IOException e) {
            log.error("I/O error during CSV to Parquet conversion and upload: {}", e.getMessage(), e);
            throw e;
        }
    }

    @Override
    public File convertCsvToParquet(List<String[]> csvData, Schema avroSchema, String fileName) throws IOException {
        File parquetFile = new File(fileName);

        Configuration hadoopConfig = new Configuration();
        hadoopConfig.set("parquet.native.enabled", "false");

        log.debug("Starting Parquet file writing. Output file: {}", fileName);
        
        int totalParquetRecords = 0;

        try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(new org.apache.hadoop.fs.Path(parquetFile.getAbsolutePath()))
                .withSchema(avroSchema)
                .withConf(hadoopConfig)
                .withValidation(false)
                .withCompressionCodec(CompressionCodecName.SNAPPY)
                .build()) {

            // Skip the first row (header row)
            boolean firstRow = true;

            for (String[] record : csvData) {
                if (firstRow) {
                    firstRow = false;
                    continue; // Skip header row
                }

                GenericRecord avroRecord = new GenericData.Record(avroSchema);

                // Map CSV columns to Avro schema fields by name
                for (Schema.Field field : avroSchema.getFields()) {
                    String fieldName = field.name();

                    // CSV headers should match Avro field names
                    int columnIndex = Arrays.asList(csvData.get(0)).indexOf(fieldName);
                    if (columnIndex == -1) {
                        log.error("CSV header '{}' not found for Avro field '{}'.", fieldName, fieldName);
                        throw new IOException("CSV header '" + fieldName + "' not found for Avro field '" + fieldName + "'");
                    }
                    String value = record[columnIndex];
                    avroRecord.put(fieldName, value);
                }

                writer.write(avroRecord);
                totalParquetRecords++;
            }

            log.info("Parquet file written successfully. File: {}", fileName);
            log.info("Total records written to Parquet file: {}", totalParquetRecords);

            // Compare the total records in CSV and Parquet data
            if (totalParquetRecords == (csvData.size() - 1)) {
                log.info("Record count matches between CSV and Parquet files.");
            } else {
                log.warn("Record count mismatch: CSV records = {}, Parquet records = {}", csvData.size() - 1, totalParquetRecords);
            }

        } catch (IOException e) {
            log.error("Error writing Parquet file: {}", e.getMessage(), e);
            throw new IOException("Error converting CSV data to Parquet", e);
        }

        return parquetFile;
    }
