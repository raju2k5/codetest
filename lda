
import software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider;
import software.amazon.awssdk.regions.Region;
import software.amazon.awssdk.services.s3.S3Client;
import software.amazon.awssdk.services.s3.model.*;
import org.apache.commons.csv.CSVFormat;
import org.apache.commons.csv.CSVParser;
import org.apache.commons.csv.CSVRecord;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.parquet.example.data.Group;
import org.apache.parquet.example.data.simple.SimpleGroupFactory;
import org.apache.parquet.hadoop.ParquetWriter;
import org.apache.parquet.hadoop.example.GroupWriteSupport;
import org.apache.parquet.schema.MessageType;
import org.apache.parquet.schema.MessageTypeParser;

import java.io.*;
import java.util.ArrayList;
import java.util.List;

public class S3CsvToParquetConverter {

    public static void main(String[] args) throws IOException {
        String bucketName = "your-bucket-name";
        String csvKey = "path/to/your/file.csv";
        String parquetKey = "path/to/output/file.parquet";

        // Initialize S3 client
        S3Client s3Client = S3Client.builder()
                .region(Region.US_EAST_1) // Replace with your desired AWS region
                .credentialsProvider(DefaultCredentialsProvider.create())
                .build();

        // Read CSV from S3
        List<String[]> records = readCsvFromS3(s3Client, bucketName, csvKey);

        // Convert CSV to Parquet
        ByteArrayOutputStream parquetOutputStream = convertCsvToParquet(records);

        // Upload Parquet to S3
        uploadParquetToS3(s3Client, bucketName, parquetKey, parquetOutputStream.toByteArray());

        System.out.println("CSV from S3 converted to Parquet and uploaded back to S3 successfully!");
    }

    public static List<String[]> readCsvFromS3(S3Client s3Client, String bucketName, String csvKey) throws IOException {
        List<String[]> records = new ArrayList<>();

        // Create a request to get object from S3
        GetObjectRequest getObjectRequest = GetObjectRequest.builder()
                .bucket(bucketName)
                .key(csvKey)
                .build();

        // Get the object from S3 as a stream
        ResponseInputStream<GetObjectResponse> objectStream = s3Client.getObject(getObjectRequest);

        // Read CSV from the object stream
        Reader reader = new InputStreamReader(objectStream);
        CSVParser csvParser = new CSVParser(reader, CSVFormat.DEFAULT);

        // Process each CSV record
        for (CSVRecord csvRecord : csvParser) {
            records.add(new String[] { csvRecord.get(0), csvRecord.get(1), csvRecord.get(2) });
        }

        // Close CSV parser and reader
        csvParser.close();
        reader.close();

        return records;
    }

    public static ByteArrayOutputStream convertCsvToParquet(List<String[]> records) throws IOException {
        // Define the Parquet schema
        MessageType schema = MessageTypeParser.parseMessageType(
                "message CsvSchema {" + "  required binary col1;" + "  required int32 col2;" + "  required double col3;"
                        + "}");

        // Prepare a ByteArrayOutputStream to hold Parquet data
        ByteArrayOutputStream baos = new ByteArrayOutputStream();

        // Configure Parquet writing
        Configuration conf = new Configuration();
        GroupWriteSupport writeSupport = new GroupWriteSupport();
        writeSupport.setSchema(schema, conf);

        ParquetWriter<Group> writer = new ParquetWriter<>(new Path("inmemory"), (WriteSupport<Group>) writeSupport,
                ParquetWriter.DEFAULT_COMPRESSION_CODEC_NAME, ParquetWriter.DEFAULT_BLOCK_SIZE,
                ParquetWriter.DEFAULT_PAGE_SIZE, ParquetWriter.DEFAULT_PAGE_SIZE,
                ParquetWriter.DEFAULT_IS_DICTIONARY_ENABLED, ParquetWriter.DEFAULT_IS_VALIDATING_ENABLED,
                ParquetWriter.DEFAULT_WRITER_VERSION, conf);

        // Convert CSV records to Parquet Groups and write them to ByteArrayOutputStream
        SimpleGroupFactory groupFactory = new SimpleGroupFactory(schema);
        for (String[] record : records) {
            Group group = groupFactory.newGroup().append("col1", record[0]).append("col2", Integer.parseInt(record[1]))
                    .append("col3", Double.parseDouble(record[2]));

            writer.write(group);
        }

        writer.close();
        baos.close();

        return baos;
    }

    public static void uploadParquetToS3(S3Client s3Client, String bucketName, String parquetKey, byte[] parquetData) {
        // Upload Parquet data to S3
        s3Client.putObject(PutObjectRequest.builder().bucket(bucketName).key(parquetKey)
                .build(), RequestBody.fromBytes(parquetData));
    }
}
