import lombok.extern.slf4j.Slf4j;
import java.io.*;
import java.util.Arrays;
import java.util.List;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;
import software.amazon.awssdk.core.sync.RequestBody;
import software.amazon.awssdk.services.s3.S3Client;
import software.amazon.awssdk.services.s3.model.GetObjectRequest;
import software.amazon.awssdk.services.s3.model.GetObjectResponse;
import software.amazon.awssdk.services.s3.model.PutObjectRequest;
import org.apache.hadoop.conf.Configuration;
import com.opencsv.CSVReader;
import com.opencsv.exceptions.CsvException;
import org.apache.avro.Schema;
import org.apache.avro.generic.GenericData;
import org.apache.avro.generic.GenericRecord;
import org.apache.parquet.avro.AvroParquetWriter;
import org.apache.parquet.hadoop.ParquetWriter;
import org.apache.parquet.hadoop.metadata.CompressionCodecName;
import software.amazon.awssdk.core.ResponseInputStream;
import software.amazon.awssdk.core.exception.SdkClientException;

/**
 * Service implementation for converting CSV data to Parquet format and uploading it to S3.
 */
@Slf4j
@Service
public class SnapshotServiceImpl implements SnapshotService {
    private final S3Client s3Client;

    @Autowired
    public SnapshotServiceImpl(final S3Client s3Client) {
        this.s3Client = s3Client;
    }

    /**
     * Converts CSV data from an S3 bucket to Parquet format and uploads the Parquet file to another S3 bucket.
     *
     * @param sourceBucketName        the name of the S3 bucket containing the source CSV file
     * @param sourceFileKey           the key (path) of the source CSV file in the S3 bucket
     * @param fileTobeProcessed       the type of file to be processed, used to determine the schema
     * @param destinationBucketName   the name of the S3 bucket where the Parquet file will be uploaded
     * @param destinationFileKy      the key (path) of the Parquet file in the destination S3 bucket
     * @throws IOException            if an I/O error occurs during processing
     * @throws SdkClientException     if an error occurs with the AWS SDK client
     */
    @Override
    public void convertCsvToParquetAndUpload(String sourceBucketName, String sourceFileKey, String fileTobeProcessed, String destinationBucketName, String destinationFileKy) throws IOException {
        String tempFileName = "/tmp/output_" + System.currentTimeMillis() + ".parquet";

        log.info("Starting the conversion of CSV to Parquet and upload process. Source: {}/{} -> Destination: {}/{}", 
                 sourceBucketName, sourceFileKey, destinationBucketName, destinationFileKy);

        try {
            // Read CSV data from S3
            log.debug("Reading CSV data from S3 bucket: {}, key: {}", sourceBucketName, sourceFileKey);
            List<String[]> csvData = readCsvFromS3(sourceBucketName, sourceFileKey);

            int totalCsvRecords = csvData.size() - 1; // Subtracting 1 for header row
            log.info("Total records in CSV data (excluding header): {}", totalCsvRecords);

            // Load the JSON schema based on fileTobeProcessed
            log.debug("Loading JSON schema for file type: {}", fileTobeProcessed);
            String jsonSchema = loadJsonSchema(fileTobeProcessed);
            Schema avroSchema = new Schema.Parser().parse(jsonSchema);

            // Convert CSV data to Parquet
            log.debug("Converting CSV data to Parquet format.");
            File parquetFile = convertCsvToParquet(csvData, avroSchema, tempFileName);

            // Upload Parquet file to S3
            log.debug("Uploading Parquet file to S3 bucket: {}, key: {}", destinationBucketName, destinationFileKey);
            String destinationFileKey = destinationFileKy.replaceAll("\\.\\w+", "") + ".parquet";
            uploadParquetToS3(destinationBucketName, destinationFileKey, parquetFile, tempFileName);

            log.info("CSV to Parquet conversion and upload completed successfully.");
        } catch (SdkClientException e) {
            log.error("AWS SDK Client error during CSV to Parquet conversion and upload: {}", e.getMessage(), e);
            throw e;
        } catch (IOException e) {
            log.error("I/O error during CSV to Parquet conversion and upload: {}", e.getMessage(), e);
            throw e;
        }
    }

    /**
     * Loads a JSON schema from the local resources.
     *
     * @param fileTobeProcessed the type of file to be processed, used to determine the schema file name
     * @return the JSON schema as a string
     * @throws IOException if the schema file cannot be found or read
     */
    @Override
    public String loadJsonSchema(String fileTobeProcessed) throws IOException {
        String schemaPath = "/schemas/" + fileTobeProcessed + ".json";
        log.debug("Loading JSON schema from path: {}", schemaPath);
        
        InputStream inputStream = getClass().getResourceAsStream(schemaPath);

        if (inputStream == null) {
            log.error("Schema file not found at path: {}", schemaPath);
            throw new FileNotFoundException("Schema file not found: " + schemaPath);
        }

        log.debug("JSON schema loaded successfully.");
        return new String(inputStream.readAllBytes());
    }

    /**
     * Reads CSV data from an S3 bucket into a list of string arrays.
     *
     * @param bucketName the name of the S3 bucket containing the CSV file
     * @param key the key (path) of the CSV file in the S3 bucket
     * @return a list of string arrays representing CSV rows
     * @throws IOException if an I/O error occurs during reading
     * @throws SdkClientException if an error occurs with the AWS SDK client
     */
    @Override
    public List<String[]> readCsvFromS3(String bucketName, String key) throws IOException, SdkClientException {
        GetObjectRequest getObjectRequest = GetObjectRequest.builder()
                .bucket(bucketName)
                .key(key)
                .build();

        log.debug("Fetching CSV file from S3 bucket: {}, key: {}", bucketName, key);
        
        try (ResponseInputStream<GetObjectResponse> objectStream = s3Client.getObject(getObjectRequest);
             Reader reader = new InputStreamReader(objectStream);
             CSVReader csvReader = new CSVReader(reader)) {

            List<String[]> csvData = csvReader.readAll();
            log.debug("CSV data successfully read from S3. Number of rows: {}", csvData.size());
            return csvData;
        } catch (CsvException e) {
            log.error("Error reading CSV data: {}", e.getMessage(), e);
            throw new IOException("Error reading CSV data from S3", e);
        } catch (SdkClientException e) {
            log.error("Error fetching CSV data from S3: {}", e.getMessage(), e);
            throw new IOException("Error fetching CSV data from S3", e);
        }
    }

    /**
     * Converts CSV data to Parquet format using the provided Avro schema and returns the resulting file.
     *
     * @param csvData a list of string arrays representing CSV rows
     * @param avroSchema the Avro schema to use for the Parquet conversion
     * @param fileName the name of the file to which the Parquet data will be written
     * @return a file representing the converted Parquet data
     * @throws IOException if an I/O error occurs during writing
     */
    @Override
    public File convertCsvToParquet(List<String[]> csvData, Schema avroSchema, String fileName) throws IOException {
        File parquetFile = new File(fileName);

        Configuration hadoopConfig = new Configuration();
        hadoopConfig.set("parquet.native.enabled", "false");

        log.debug("Starting Parquet file writing. Output file: {}", fileName);
        
        int totalParquetRecords = 0;

        try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(new org.apache.hadoop.fs.Path(parquetFile.getAbsolutePath()))
                .withSchema(avroSchema)
                .withConf(hadoopConfig)
                .withValidation(false)
                .withCompressionCodec(CompressionCodecName.SNAPPY)
                .build()) {

            // Skip the first row (header row)
            boolean firstRow = true;

            for (String[] record : csvData) {
                if (firstRow) {
                    firstRow = false;
                    continue; // Skip header row
                }

                GenericRecord avroRecord = new GenericData.Record(avroSchema);

                // Map CSV columns to Avro schema fields by name
                for (Schema.Field field : avroSchema.getFields()) {
                    String fieldName = field.name();

                    // CSV headers should match Avro field names
                    int columnIndex = Arrays.asList(csvData.get(0)).indexOf(fieldName);
                    if (columnIndex == -1) {
                        log.error("CSV header '{}' not found for Avro field '{}'.", fieldName, fieldName);
                        throw new IOException("CSV header '" + fieldName + "' not found for Avro field '" + fieldName + "'");
                    }
                    String value = record[columnIndex];
                    avroRecord.put(fieldName, value);
                }

                writer.write(avroRecord);
                totalParquetRecords++;
            }

            log.info("Parquet file written successfully. File: {}", fileName);
            log.info("Total records written to Parquet file: {}", totalParquetRecords);

            // Compare the total records in CSV and Parquet data
            if (totalParquetRecords == (csvData.size() - 1)) {
                log.info("Record count matches between CSV and Parquet files.");
            } else {
                log.warn("Record count mismatch: CSV records = {}, Parquet records = {}", csvData.size() - 1, totalParquetRecords);
            }

        } catch (IOException e) {
            log.error("Error writing Parquet file: {}", e.getMessage(), e);
            throw new IOException("Error converting CSV data to Parquet", e);
        }

        return parquetFile;
    }

    /**
     * Uploads a Parquet file to an S3 bucket and deletes the local temporary file.
     *
     * @param destinationBucketName the name of the S3 bucket where the Parquet file will be uploaded
     * @param destinationFileKey the key (path) of the Parquet file in the S3 bucket
     * @param parquetFile the file representing Parquet data to be uploaded
     * @param tempFileName the name of the temporary file to be deleted after upload
     * @throws SdkClientException if an error occurs with the AWS SDK client during the upload
     */
    @Override
    public void uploadParquetToS3(String destinationBucketName, String destinationFileKey, File parquetFile, String tempFileName) {
        log.debug("Uploading Parquet file to S3 bucket: {}, key: {}", destinationBucketName, destinationFileKey);
        
        try {
            s3Client.putObject(PutObjectRequest.builder()
                    .bucket(destinationBucketName)
                    .key(destinationFileKey)
                    .build(), RequestBody.fromFile(parquetFile));

            log.info("Parquet file uploaded successfully to S3: {}/{}", destinationBucketName, destinationFileKey);
            deleteSysParquetFile(tempFileName);
        } catch (SdkClientException e) {
            log.error("Error uploading Parquet file to S3: {}", e.getMessage(), e);
            throw new RuntimeException("Error uploading Parquet file to S3: " + e.getMessage(), e);
        }
    }

    /**
     * Deletes the local temporary Parquet file and its corresponding CRC file.
     *
     * @param parquetFileName the name of the temporary Parquet file to be deleted
     */
    @Override
    public void deleteSysParquetFile(String parquetFileName) {
        log.debug("Deleting temporary Parquet file: {}", parquetFileName);
        
        File localParquetFile = new File(parquetFileName);
        File localCrcFile = new File("." + parquetFileName + ".crc");

        if (localParquetFile.exists() && localParquetFile.delete()) {
            log.info("Parquet file deleted successfully: {}", parquetFileName);
        } else {
            log.warn("Parquet file not found or could not be deleted: {}", parquetFileName);
        }

        if (localCrcFile.exists() && localCrcFile.delete()) {
            log.info(".crc file deleted successfully: {}", localCrcFile.getName());
        } else {
            log.warn(".crc file not found or could not be deleted: {}", localCrcFile.getName());
        }
    }
}
